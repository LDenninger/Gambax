from openai import OpenAI
import logging
import socket
from abc import abstractmethod
from pathlib import Path
import os
from pydantic import BaseModel
import subprocess
import json
from typing import Tuple, List, Union, Dict, Literal
logger = logging.getLogger("Gambax Coder")

from gambax.services import Service
from gambax.models import ModelInterface
from gambax.utils import message_template, setup_logger

SYSTEM_CALL_CODER = """
    You are an expert coding AI specializing in generating correct, optimized, and well-structured code based on user requests.
    Your responses must strictly adhere to the following guidelines:
    Guidelines:
        - Always generate fully functional, optimized, and efficient code.
        - The response must be a complete script, ready to run without modifications.
        - Always include required correct import statements at the beginning of the script.
        - Do not use any interactive I/O output or display output.
        - Produce a separate file that includes unit tests, ensuring the full functionality of the generated code.
        - The test file must:
            Use an appropriate testing framework (e.g., unittest for Python, Jest for JavaScript, etc.).
            Cover edge cases, common use cases, and potential failure scenarios.
            Verify that the generated code works as expected.
        - Include a run command that runs the tests for the generated code.
        - The files will be placed in a common directory, which will be the working directory.
        - Name the main script 'code_script.py' and the test file name will be 'code_script_test.py'
        - Import the main script as `from code_script import *` in your test file.
"""
SYSTEM_CALL_ANALYZER = """
    You are an Expert AI Coding Analyst tasked with analyzing and refining code previously generated by another AI model.
    Your role is to assess the generated scripts and extract only the relevant parts as per the user's request, ensuring code integrity and usability.

    Processing Guidelines:

        Granularity Rule:
            The smallest unit of returned code should be a complete function.
            Avoid returning fragmented or incomplete code blocks.

        Code Integrity Preference:
            Prioritize the integrity of functional units over isolated code snippets.
            If a function references variables, imports, or dependencies, include those for completeness.

        Determining Scope of Output:
            If the userâ€™s request implies a need for a full script, return the entire script instead of extracting portions.
            If the request is explicitly about specific functions or components, return only those complete functions.

        Maintaining Contextual Relevance:
            Ensure extracted functions or scripts retain the necessary context to function correctly.
            Include dependent helper functions if they are critical for execution.

        Code Formatting & Readability:
            Maintain consistent indentation and formatting to enhance readability.
            Ensure proper syntax and structure are preserved.
"""

SYSTEM_CALL_ERROR_ANAALYZER = """
    You are an advanced AI assistant tasked with debugging and analyzing errors in code. Your job is to examine the provided code and the accompanying error message, then determine whether you can fix the issue yourself or if the user should be given back the code with clear instructions on how to fix it.

    **Process:**
    1. Carefully analyze the error message and the provided code to understand the root cause.
    2. The test code was also generated and should be included in possible fixes.
    3. Whenever possible, automatically fix errors that do not require user-specific knowledge.
    4. If the issue involves external dependencies, business logic decisions, or configurations beyond the provided code, attempt to infer reasonable default values or solutions before requesting user input.
    5. Only return the code to the user if:
    - The issue requires explicit business logic or domain knowledge that cannot be reasonably inferred.
    - A critical dependency is missing and cannot be substituted with a default or fallback solution.
    - The fix has multiple valid approaches that depend on the user's intent.
    6. If a user-specific decision is necessary, exclude an in-depth analysis but provide **clear, concise, and actionable** instructions on how to resolve the issue.
    7. If the fix is feasible without ambiguity, proceed with the correction.

    **Output Structure:**
    You must return a structured JSON response with the function `decide_to_continue`. This function contains:
    - `"analysis"`: A detailed breakdown of the error cause, possible fixes, and any additional insights.
    - `"should_continue"`: A boolean flag that is `true` if you can safely fix the error yourself and `false` if the user should make the necessary changes.

    **Guidelines:**
    - Favor **autonomous fixes** whenever possible.
    - Ensure the fix does not introduce unintended side effects.
    - If `should_continue` is `false`, provide a **minimal but precise** step-by-step guide for the user.
    - If `should_continue` is `true`, the AI must **apply the fix directly** instead of just suggesting it.
"""

functions_prompt_engineer = [
    {
        "name": "return_refined_prompt",
        "description": "Returns the refined prompt.",
        "parameters": {
            "type": "object",
            "properties": {
                "prompt": {
                    "type": "string",
                    "description": "The refined prompt." 
                }
            },
            "required": ["prompt"]
        }
    }
]


functions = [
    {
        "name": "return_generated_code",
        "description": "Returns the structured data with the required fields.",
        "parameters": {
            "type": "object",
            "properties": {
                "file_name": {
                    "type": "string",
                    "description": "The file name for the generated code." 
                },
                "code": {
                    "type": "string",
                    "description": "The generated code as a complete python script."
                },
                "test": {
                    "type": "string",
                    "description": "The test file"
                },
                "run_command": {
                    "type": "string",
                    "description": "A string describing any lines that follow."
                }
            },
            "required": ["file_name", "code", "test", "run_command"]
        }
    }
]

functions_error_analyzer = [
    {
        "name": "decide_to_continue",
        "description": "Analyzes an error and determines whether it can be fixed automatically or should be returned to the user with instructions.",
        "parameters": {
            "type": "object",
            "properties": {
                "analysis": {
                    "type": "string",
                    "description": "A detailed explanation of the error, its root cause, and how it can be resolved."
                },
                "should_continue": {
                    "type": "boolean",
                    "description": "Indicates whether the AI should proceed with fixing the error (`true`) or return it to the user for manual correction (`false`)."
                }
            },
            "required": ["analysis", "should_continue"]
        }
    }
]

functions_analyzer = [
    {
        "name": "return_cleaned_code",
        "description": "Returns the cleaned code.",
        "parameters": {
            "type": "object",
            "properties": {
                "code": {
                    "type": "string",
                    "description": "The final generated code."
                }
            },
            "required": ["code"]
        }
    }
]

class Verifier:

    def __init__(self):
        return
    
    @abstractmethod
    def verify(self, *args, **kwargs) -> Tuple[bool, str]:
        raise NotImplementedError(f"Verifier '{self.__class__.__name__}' must implement 'verify()' method")

class GambaxCoder(Service):

    def __init__(self, 
                 coding_model= "gpt-4o",
                 chat_model= "gpt-4o-mini",
                 prompt_stage: bool = True,
                 verify_stage: bool = True,
                 refine_iterations: int = 2,
                 refine_stage: bool = None,
                 use_project_context: bool = True,
                 verifier: Literal['test_suite'] = None,
                 verifier_host: str = "127.0.0.1",
                 verifier_port: str = "8080",
                 debug: bool = True,
                 ):
        super().__init__(name="gambax_coder")
        self.coding_model = coding_model
        self.chat_model = chat_model
        self.logger = setup_logger("GambaxCoder", log_level=logging.DEBUG if debug else logging.INFO)
        self.prompt_stage = prompt_stage
        self.verify_stage = verify_stage
        self.refine_iterations = refine_iterations
        self.refine_stage = refine_stage
        self.use_project_context = use_project_context
        self.verifier = self._setup_verifier(verifier) if verifier else None

        self.verifier_host = verifier_host
        self.verifier_port = verifier_port
        
        self.client = OpenAI()

    def _setup_verifier(self, verifier: str):
        if verifier == 'test_suite':
            return GambaxTestSuiteVerifier(
                logger=self.logger,
            )
        else:
            raise ValueError(f"Invalid verifier type '{verifier}'. Expected 'test_suite'")

    def request_impl(self,
                      prompt: str,
                      context: str = "",
                      prompt_stage: bool = None,
                      verify_stage: bool = None,
                      refine_iterations: int = None,
                      use_project_context: bool = None,
                      refine_stage: bool = None,
                      ):
        prompt_stage = prompt_stage or self.prompt_stage
        verify_stage = verify_stage or self.verify_stage
        refine_iterations = refine_iterations or self.refine_iterations
        use_project_context = use_project_context or self.use_project_context
        refine_stage = refine_stage or self.refine_stage

        code = ""
        explanation = ""

        if prompt_stage:
            self.logger.info("Refining prompt for coding...")
            messages = [
                message_template("system", f"You are an AI prompt engineer, that receives prompts for a coding agents and refines them to a clearer, concise and structured prompt such that the model '{self.coding_model} generates high quality code given this command'"),
                message_template("user", prompt)
            ]
            response = self.client.chat.completions.create(
                model=self.chat_model,
                messages=messages,
                temperature=0.6,
                max_tokens=2000,
                top_p=1.0,
                functions=functions_prompt_engineer,
                function_call={"name": "return_refined_prompt"}
            )
            function_call = response.choices[0].message.function_call
            arguments = json.loads(function_call.arguments)
            prompt = arguments['prompt']

        messages = [
            message_template('system', SYSTEM_CALL_CODER),
            message_template('user', context),
            message_template('user', prompt)
        ]

        should_continue = True
        iter = 0
        while should_continue:
            if iter >= self.refine_iterations:
                self.logger.info("Reached maximum re-try iterations.")
                break
            iter += 1
            self.logger.info("Generating code...")
            response = self.client.chat.completions.create(
                    model=self.coding_model,
                    messages=messages,
                    temperature=0.6,
                    max_tokens=2000,
                    top_p=1.0,
                    functions=functions,
                    function_call={"name": "return_generated_code"}
            )
            function_call = response.choices[0].message.function_call
            arguments = json.loads(function_call.arguments)
            valid, reason = self.verifier.verify(**arguments) if self.verifier else True

            if valid:
                should_continue = False
                continue
            messages += [
                message_template("user", f"The provided code failed the tests and does not work. Correct your solution. Do not hesitate to scratch your initial solution and to re-try."),
                message_template("user", reason)
                ]
            
            code = arguments['code']
            
            error_analyzer_messages = [
                message_template('system', SYSTEM_CALL_ERROR_ANAALYZER),
                message_template('user', f"Run Command: {arguments['run_command']}"),
                message_template('user', f"Generated Code:\n{arguments['code']}"),
                message_template('user', f"Test Code:\n{arguments['test']}"),

            ]
            response = self.client.chat.completions.create(
                    model=self.coding_model,
                    messages=error_analyzer_messages,
                    temperature=0.0,
                    max_tokens=2000,
                    top_p=1.0,
                    functions=functions_error_analyzer,
                    function_call={"name": "decide_to_continue"}
            )
            #import ipdb; ipdb.set_trace()
            function_call = response.choices[0].message.function_call
            arguments = json.loads(function_call.arguments)
            if not arguments['should_continue']:
                refine_stage = False
                should_continue = False
                explanation = arguments['analysis']
                self.logger.info(f"Verification failed. User input required!")
                continue
            
            self.logger.info(f"Verification failed. Retrying...")


        #import ipdb; ipdb.set_trace()
        if refine_stage:
            messages = [
                message_template("system", SYSTEM_CALL_ANALYZER),
                message_template("user", prompt),
                message_template("assistant", arguments['code']),
            ]
            response = self.client.chat.completions.create(
                    model=self.coding_model,
                    messages=messages,
                    temperature=0.0,
                    max_tokens=2000,
                    top_p=1.0,
                    functions=functions_analyzer,
                    function_call={"name": "return_cleaned_code"}
            )
            function_call = response.choices[0].message.function_call
            arguments = json.loads(function_call.arguments)
            code = arguments['code']

        return {
            "code": code,
            "explanation": explanation
        } 


    @property
    def verifier_reachable(self):
        socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        socket.settimeout(2)  # Set timeout for quick response
        
        result = socket.connect_ex((self.verifier_host, self.verifier_port))  # Returns 0 if reachable
        socket.close()
        
        return result == 0
    


class GambaxTestSuiteVerifier(Verifier):

    def __init__(self, 
                 workspace_path: str = '.tmp',
                 environment_setup: List[str] = ['source ~/.zshrc', 'source_conda', "conda activate main"],
                 executable: str = "/bin/zsh",
                 logger: logging.Logger = logging.getLogger(),
                 debug: bool = False,
                 ):
        
        self.workspace_path = Path(workspace_path)
        self.environment_setup = environment_setup
        self.executable = executable

        self.logger = logger

        os.makedirs(self.workspace_path, exist_ok=True)
        with open(self.workspace_path / "__init__.py", "w") as file:
            file.write("")

    def verify(self, file_name: str, code: str, test: str, run_command: str) -> Tuple[List[bool], List[str]]:
        file_valid = False
        reason = ""

        file_path = self.workspace_path / file_name
        test_file_path = self.workspace_path / f"{file_name.split('.')[0]}_test.py"
        try:
            with open(file_path, "w") as file:
                file.write(code)
            with open(test_file_path, "w") as file:
                file.write(test)
        except Exception as e:
            self.logger.error(f"Error writing file '{file_name}': {str(e)}")

        env_setup_cmd = ";".join(self.environment_setup)
        run_cmd = f"cd {self.workspace_path};{env_setup_cmd}; {run_command}"
        
        try:
            result = subprocess.run(run_cmd, shell=True, capture_output=True, executable=self.executable, text=True)
            if result.returncode == 0:
                file_valid = True
            else:
                reason = f"stderr:\n{result.stderr}"
                self.logger.debug(f"Test failed:\n{str(result.stderr)}")

        except Exception as e:
            self.logger.error(f"Error running command '{run_command}': {str(e)}")
            reason = f"Error occurred while running command"
        
        return file_valid, reason

            

            
